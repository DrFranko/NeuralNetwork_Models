{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4836070c-b6f3-43e6-a571-ee337995c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4a884-ff96-4bc1-97e4-d4ee2160fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_S(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,n_layers,bias):\n",
    "        super(LSTM_S,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.output_size=output_size\n",
    "        self.n_layers=n_layers\n",
    "        self.bias=bias\n",
    "\n",
    "        self.layers=nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            layer_input_size = input_size if i == 0 else hidden_size\n",
    "            layer = nn.Module()\n",
    "            layer.wlr1 = nn.Parameter(torch.rand(hidden_size), requires_grad=True)\n",
    "            layer.wlr2 = nn.Parameter(torch.rand(layer_input_size), requires_grad=True)\n",
    "            layer.blr1 = nn.Parameter(torch.rand(hidden_size), requires_grad=True)\n",
    "            layer.wpr1 = nn.Parameter(torch.rand(hidden_size), requires_grad=True)\n",
    "            layer.wpr2 = nn.Parameter(torch.rand(layer_input_size), requires_grad=True)\n",
    "            layer.bpr1 = nn.Parameter(torch.rand(hidden_size), requires_grad=True)\n",
    "            layer.wp1 = nn.Parameter(torch.rand(hidden_size), requires_grad=True)\n",
    "            layer.wp2 = nn.Parameter(torch.rand(layer_input_size), requires_grad=True)\n",
    "            layer.bp1 = nn.Parameter(torch.rand(hidden_size), requires_grad=True)\n",
    "            layer.wo1 = nn.Parameter(torch.rand(hidden_size), requires_grad=True)\n",
    "            layer.wo2 = nn.Parameter(torch.rand(layer_input_size), requires_grad=True)\n",
    "            layer.bo1 = nn.Parameter(torch.rand(hidden_size), requires_grad=True)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def unit_cal(self, layer, input, long_mm, short_mm):\n",
    "        First_stage = torch.sigmoid(torch.mm(short_mm, layer.wlr1.t()) + torch.mm(input, layer.wlr2.t()) + layer.blr1)\n",
    "        Second_stage = torch.sigmoid(torch.mm(short_mm, layer.wpr1.t()) + torch.mm(input, layer.wpr2.t()) + layer.bpr1)\n",
    "        Second_memory = torch.tanh(torch.mm(short_mm, layer.wp1.t()) + torch.mm(input, layer.wp2.t()) + layer.bp1)\n",
    "        updated_long_memory = ((long_mm * First_stage) + (Second_stage * Second_memory))\n",
    "        output_percent = torch.sigmoid(torch.mm(short_mm, layer.wo1.t()) + torch.mm(input, layer.wo2.t()) + layer.bo1)         \n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "\n",
    "        return updated_long_memory, updated_short_memory\n",
    "    \n",
    "    def forward(self, input, hx=None):\n",
    "        batch_size, seq_len, _ = input.size()\n",
    "        \n",
    "        if hx is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "            if input.is_cuda:\n",
    "                h0 = h0.cuda()\n",
    "                c0 = c0.cuda()\n",
    "        else:\n",
    "            h0, c0 = hx\n",
    "        \n",
    "        outs = []\n",
    "        hidden = [(h0[i], c0[i]) for i in range(self.num_layers)]\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            for layer in range(self.num_layers):\n",
    "                if layer == 0:\n",
    "                    layer_input = input[:, t, :]\n",
    "                else:\n",
    "                    layer_input = hidden[layer-1][1]  \n",
    "                \n",
    "                long_memory, short_memory = self.unit_cal(\n",
    "                    self.layers[layer],\n",
    "                    layer_input,\n",
    "                    hidden[layer][0], \n",
    "                    hidden[layer][1]   \n",
    "                )\n",
    "                hidden[layer] = (long_memory, short_memory)\n",
    "            \n",
    "            outs.append(hidden[-1][1]) \n",
    "        \n",
    "        out = outs[-1].squeeze()\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
